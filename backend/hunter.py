from playwright.async_api import async_playwright
import re
import urllib.parse
import unicodedata
import asyncio

def remove_accents(input_str):
    if not input_str:
        return ""
    nfkd_form = unicodedata.normalize('NFKD', input_str)
    return "".join([c for c in nfkd_form if not unicodedata.combining(c)])

async def hunt_emails_on_web(domain: str):
    """
    Estrat√©gia Deep Harvest (For√ßa Bruta V2):
    1. Queries Globais (sem restri√ß√£o de BR).
    2. Filtragem Ativa de Lixo (jobs, company, pulse).
    3. Parser Tolerante a Falhas.
    """
    clean_domain = domain.replace("http://", "").replace("https://", "").replace("www.", "").split("/")[0]
    company_raw = clean_domain.split('.')[0] # ex: opentreinamentos
    
    # --- LISTA DE TENTATIVAS DE BUSCA ---
    # Removido "br." para pegar perfis globais e evitar filtro excessivo
    search_queries = [
        f'site:linkedin.com/in/ "{clean_domain}"',       # Dom√≠nio exato no perfil
        f'site:linkedin.com/in/ "{company_raw}"',        # Nome da empresa (aspas)
        f'site:linkedin.com/in/ {company_raw} -intitle:jobs -intitle:company', # Nome solto + filtros
        f'"{company_raw}" site:linkedin.com/in/ email'    # Tentativa de achar quem exp√µe email
    ]
    
    found_leads = []
    seen_keys = set() 

    # --- 1. Gen√©ricos (Base de seguran√ßa) ---
    common_prefixes = ["contato", "comercial", "financeiro", "rh", "vendas", "adm", "suporte", "diretoria"]
    for prefix in common_prefixes:
        email = f"{prefix}@{clean_domain}"
        key = email
        if key not in seen_keys:
            found_leads.append({
                "name": prefix.capitalize(),
                "email": email,
                "linkedin": None,
                "role": "Departamento"
            })
            seen_keys.add(key)

    # --- 2. Busca Profunda no Google ---
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False) # Mantenha False para debug visual se necess√°rio
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Iniciando Varredura Profunda V2 para: {clean_domain}")

        for query in search_queries:
            # Se j√° temos uma boa quantidade de leads reais (ex: 15), pula o resto
            real_people = [l for l in found_leads if l['linkedin']]
            if len(real_people) >= 15:
                break

            print(f"   ‚Ü≥ Tentando query: {query}")
            
            try:
                encoded_query = urllib.parse.quote(query)
                # num=100 para pegar m√°ximo poss√≠vel por p√°gina
                google_url = f"https://www.google.com/search?q={encoded_query}&num=100&hl=pt-BR"
                
                await page.goto(google_url, timeout=30000)
                await asyncio.sleep(3 + (len(found_leads) * 0.1)) # Delay din√¢mico leve

                # Check Captcha
                if await page.locator("text=recaptcha").count() > 0:
                    print("‚ö†Ô∏è CAPTCHA! Resolva manualmente...")
                    while await page.locator("text=recaptcha").count() > 0:
                        await asyncio.sleep(1)
                    await asyncio.sleep(2)

                # Coletar Links
                all_links = await page.locator("a").all()
                extracted_count = 0
                
                for link in all_links:
                    try:
                        href = await link.get_attribute("href")
                        if not href or "linkedin.com/in/" not in href:
                            continue

                        # Filtros de URL suja
                        if any(x in href for x in ["/jobs/", "/company/", "/pulse/", "/dir/", "/learning/", "/posts/"]):
                            continue

                        title = await link.inner_text()
                        if not title: continue

                        # Limpeza do T√≠tulo
                        # Google costuma retornar: "Nome Sobrenome - Cargo - Empresa | LinkedIn"
                        # Ou: "Nome Sobrenome | LinkedIn"
                        clean_title = title
                        for suffix in [" - LinkedIn", " | LinkedIn", " | LinkedIn Brasil"]:
                            clean_title = clean_title.split(suffix)[0]
                        clean_title = clean_title.replace("...", "").strip()

                        # Filtros de T√≠tulo sujo (termos gen√©ricos que aparecem na busca)
                        junk_terms = ["perfil", "login", "cadastre-se", "vagas", "pessoas tamb√©m viram", "outros perfis", "traduzir esta p√°gina"]
                        if any(term in clean_title.lower() for term in junk_terms):
                            continue

                        # Parser de Nome e Cargo
                        # Tenta quebrar por separadores comuns
                        separators = [" - ", " ‚Äì ", " | ", ","]
                        name_raw = clean_title
                        role_raw = "Funcion√°rio"

                        found_sep = False
                        for sep in separators:
                            if sep in clean_title:
                                parts = clean_title.split(sep)
                                name_raw = parts[0].strip()
                                # O resto √© cargo/empresa
                                role_full = parts[1].strip()
                                # Tenta limpar empresa do cargo (ex: "Gerente na Open")
                                role_raw = role_full.split(" na ")[0].split(" da ")[0].split(" at ")[0].strip()
                                found_sep = True
                                break
                        
                        # Se n√£o achou separador, assume que o t√≠tulo inteiro √© o nome (comum em perfis sem cargo no t√≠tulo)
                        
                        # Valida√ß√£o de Nome (M√≠nimo 2 partes, sem n√∫meros)
                        if len(name_raw.split()) < 2 or any(char.isdigit() for char in name_raw):
                            continue

                        # Gera√ß√£o de E-mail
                        name_parts = name_raw.split()
                        first = remove_accents(name_parts[0].lower())
                        last = remove_accents(name_parts[-1].lower()) # Pega o √∫ltimo sobrenome para garantir
                        
                        # Estrat√©gia: primeiro.ultimo
                        generated_email = f"{first}.{last}@{clean_domain}"
                        
                        # Deduplica√ß√£o baseada no LinkedIn (mais confi√°vel que email gerado)
                        if href not in seen_keys:
                            print(f"      üë§ Capturado: {name_raw} -> {role_raw}")
                            found_leads.append({
                                "name": name_raw,
                                "email": generated_email,
                                "linkedin": href,
                                "role": role_raw
                            })
                            seen_keys.add(href)
                            seen_keys.add(generated_email) # Evita gerar o mesmo email para pessoas diferentes (colis√£o simples)
                            extracted_count += 1
                            
                    except Exception as e:
                        # print(f"Erro item: {e}")
                        continue

                print(f"      ‚úÖ Extra√≠dos nesta p√°gina: {extracted_count}")

            except Exception as e:
                print(f"‚ö†Ô∏è Erro query '{query}': {e}")
                continue

        await browser.close()

    print(f"üèÅ Varredura finalizada. Total de leads: {len(found_leads)}")
    return found_leads